{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define instruments (IRMAS has 11 categories)\n",
    "INSTRUMENTS = ['cello', 'clarinere', 'flauta', 'guitara acustica', 'guitara electrica', 'órgano', 'piano', 'saxofon', 'trompeta', 'violin', 'voz', 'tambores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.png'):\n",
    "            spectrograms.append(os.path.join(data_dir, file))\n",
    "            label_file = os.path.join(data_dir, f\"{Path(file).stem}.npy\")\n",
    "            labels.append(np.load(label_file))\n",
    "    return spectrograms, labels\n",
    "\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "def data_generator(spectrogram_paths, labels):\n",
    "    for path, label in zip(spectrogram_paths, labels):\n",
    "        # Load image and label\n",
    "        image = tf.image.decode_png(tf.io.read_file(path), channels=3)\n",
    "        image = tf.image.resize(image, (128, 128)) / 255.0\n",
    "        yield image, tf.convert_to_tensor(label, dtype=tf.float32)\n",
    "\n",
    "spectrograms, labels = load_dataset(\"procesado_train\")  \n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(spectrograms, labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(len(INSTRUMENTS) + 1,), dtype=tf.float32)  # +1 for drum label\n",
    "    )\n",
    ").batch(32).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "spectrograms, labels = load_dataset(\"procesado_valid\")  \n",
    "\n",
    "valid_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(spectrograms, labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(len(INSTRUMENTS) + 1,), dtype=tf.float32)  # +1 for drum label\n",
    "    )\n",
    ").batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
    "   \n",
    "    # Compute precision and recall manually to avoid state-related issues\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred_binary)\n",
    "    predicted_positives = tf.reduce_sum(y_pred_binary)\n",
    "    actual_positives = tf.reduce_sum(y_true)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positives / (actual_positives + tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Compute F1 Score\n",
    "    f1_score = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "   \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(INSTRUMENTS) + 1, activation='sigmoid')  # +1 for drum label\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),  # Multi-label loss\n",
    "              metrics=[f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The dataset is infinite.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\joses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:531\u001b[0m, in \u001b[0;36mDatasetV2.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcardinality()\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m INFINITE:\n\u001b[1;32m--> 531\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is infinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m UNKNOWN:\n\u001b[0;32m    533\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset length is unknown.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: The dataset is infinite."
     ]
    }
   ],
   "source": [
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Epoch 1/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 434ms/step - f1_score: 0.0985 - loss: 0.3763 - val_f1_score: 0.3122 - val_loss: 0.2910\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 421ms/step - f1_score: 0.3425 - loss: 0.3018 - val_f1_score: 0.5527 - val_loss: 0.2474\n",
      "Epoch 3/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 425ms/step - f1_score: 0.5163 - loss: 0.2479 - val_f1_score: 0.6260 - val_loss: 0.2006\n",
      "Epoch 4/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 443ms/step - f1_score: 0.6625 - loss: 0.1924 - val_f1_score: 0.7926 - val_loss: 0.1224\n",
      "Epoch 5/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 416ms/step - f1_score: 0.8176 - loss: 0.1146 - val_f1_score: 0.9068 - val_loss: 0.0414\n",
      "Epoch 6/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 416ms/step - f1_score: 0.9558 - loss: 0.0371 - val_f1_score: 0.9512 - val_loss: 0.0113\n",
      "Epoch 7/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 414ms/step - f1_score: 0.9827 - loss: 0.0152 - val_f1_score: 0.9471 - val_loss: 0.0136\n",
      "Epoch 8/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 419ms/step - f1_score: 0.9893 - loss: 0.0117 - val_f1_score: 0.9537 - val_loss: 0.0043\n",
      "Epoch 9/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 428ms/step - f1_score: 0.9929 - loss: 0.0073 - val_f1_score: 0.9537 - val_loss: 0.0037\n",
      "Epoch 10/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 420ms/step - f1_score: 0.9946 - loss: 0.0047 - val_f1_score: 0.9533 - val_loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "spe = ceil(6705/32)\n",
    "print(spe)\n",
    "history = model.fit(train_ds, validation_data=valid_ds,steps_per_epoch=spe, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms, labels = load_dataset(\"procesado_test\")  \n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(spectrograms, labels),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(len(INSTRUMENTS) + 1,), dtype=tf.float32)  # +1 for drum label\n",
    "    )\n",
    ").batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - f1_score: 0.4056 - loss: 1.1587\n",
      "Test Loss: 1.3777152299880981\n",
      "Test Accuracy: 35.06467342376709%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"filepath=Modelo1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow dataset\n",
    "def single_wav(wav_path, sr=22050, n_fft=2048, hop_length=512):\n",
    "    \n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    \n",
    "    # Compute Mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Save as image\n",
    "    save_path = \"tests/spectograms/\" + os.path.splitext(wav_path[11:])[0] + \".png\"\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    librosa.display.specshow(S_dB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    image = tf.image.decode_png(tf.io.read_file(save_path), channels=3)\n",
    "    image = tf.image.resize(image, (128, 128)) / 255.0\n",
    "    image = tf.expand_dims(image, axis = 0)\n",
    "    return image\n",
    "\n",
    "def single_spectogram(spectrogram_path):\n",
    "    image = tf.image.decode_png(tf.io.read_file(spectrogram_path), channels=3)\n",
    "    image = tf.image.resize(image, (128, 128)) / 255.0\n",
    "    image = tf.expand_dims(image, axis = 0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "[[3.8157744e-08 8.6779464e-09 3.5542928e-07 7.0441933e-03 9.9747986e-01\n",
      "  6.3722888e-05 3.1407146e-09 6.7306075e-08 1.9849665e-08 6.9293094e-07\n",
      "  7.6669294e-01 9.9907682e-17]]\n"
     ]
    }
   ],
   "source": [
    "# Path to a test WAV file\n",
    "file_path = \"procesado_test/00 - gold fronts-10.png\"\n",
    "\n",
    "# Preprocess the file\n",
    "input_data = single_spectogram(file_path)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(input_data)\n",
    "\n",
    "# Output predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrumento de mayor predominancia: guitara electrica\n",
      "Instrumentos detectados: ['guitara electrica']\n"
     ]
    }
   ],
   "source": [
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "predicted_label = INSTRUMENTS[predicted_class]\n",
    "print(f\"Instrumento de mayor predominancia: {predicted_label}\")\n",
    "\n",
    "threshold = 0.8\n",
    "multi_labels = [INSTRUMENTS[i] for i, prob in enumerate(predictions[0]) if prob > threshold]\n",
    "print(f\"Instrumentos detectados: {multi_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
